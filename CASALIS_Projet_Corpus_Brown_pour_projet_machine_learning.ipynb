{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown as br\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk import plaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories={}\n",
    "categories[\"news\"] = ['news', 'editorial', 'reviews']\n",
    "categories[\"books\"] = ['adventure', 'fiction', 'mystery', 'romance']\n",
    "categories[\"sciences\"] = ['learned', 'government']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for category in categories:\n",
    "    corpus[category] = br.fileids(categories=categories[category])        \n",
    "#le corpus est constitué des IDs de chaque texte\n",
    "#print corpus #Tous les id sont affichés et placés au bon endroit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico_taille = {}\n",
    "for categorie in ['news', 'books', 'sciences']:\n",
    "    nombre_fichiers = len(corpus[categorie])\n",
    "    dico_taille[categorie] = int(nombre_fichiers * (80.0/100.0))\n",
    "#un dico avec la taille de chaque sous corpus est créé.\n",
    "dico_taille['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "for categorie in corpus :\n",
    "    train_set[categorie]=[]\n",
    "    for i in range(0, dico_taille[categorie]):\n",
    "        fichier = random.choice(corpus[categorie])         #les textes sont choisis aléatoirement\n",
    "        train_set[categorie].append(fichier)\n",
    "        corpus[categorie].remove(fichier) # on retire le fichier du corpus de départ\n",
    "test_set = corpus\n",
    "#les corpus sont prêts à l'emploi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######partition terminée ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#suppression des majuscules et autres\n",
    "for categorie in train_set:\n",
    "    for i in range(0, len(train_set[categorie])):\n",
    "        texte = [mot.lower() for mot in  train_set[categorie][i][1] if mot not in \"\"\"`'\\\"(),;.?:!\"\"\" and mot not in ['--', \"''\", \"``\"]]\n",
    "        train_set[categorie][i] = [train_set[categorie][i],texte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for categorie in test_set:\n",
    "    for i in range(0, len(test_set[categorie])):\n",
    "        texte = [mot.lower() for mot in  test_set[categorie][i][1] if mot not in \"\"\"`'\\\"(),;.?:!\"\"\" and mot not in ['--', \"''\", \"``\"]]\n",
    "        test_set[categorie][i] = [test_set[categorie][i],texte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "######################## Création des features ##################################\n",
    "#################################################################################\n",
    "\n",
    "textes_news = {}\n",
    "for i in range(0, len(train_set['news'])):\n",
    "    for file in train_set['news']:\n",
    "        for mot in train_set['news'][i][1]:\n",
    "            if mot not in textes_news:\n",
    "                textes_news[mot] = 1\n",
    "            else:\n",
    "                textes_news[mot] += 1\n",
    "\n",
    "textes_litt = {}\n",
    "for i in range(0, len(train_set['books'])):\n",
    "    for mot in train_set['books'][i][1]:\n",
    "        if mot not in textes_litt:\n",
    "            textes_litt[mot] = 1\n",
    "        else:\n",
    "            textes_litt[mot] += 1\n",
    "\n",
    "\n",
    "textes_sci = {}\n",
    "for i in range(0, len(train_set['sciences'])):\n",
    "    for mot in train_set['sciences'][i][1]:\n",
    "        if mot not in textes_sci:\n",
    "            textes_sci[mot] = 1\n",
    "        else:\n",
    "            textes_sci[mot] += 1\n",
    "\n",
    "\n",
    "###########################################################\n",
    "#Champ lexical a été établi pour chaque catégorie du train_set\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on cherche maintenant à faire un dictionnaire global\n",
    "#Où je compte le nombre de fois où chaque mot apparait pour chaque texte\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tentative donner fréquence d'apparition de chaque mot pour chaque fichier\n",
    "nb_mots_news = []\n",
    "for i in range (0, len(test_set['news'])):\n",
    "    for br.words in textes_news\n",
    "        nb_sents_news = nltk.freqDist(textes_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################################################\n",
    "# On établit un sac de mots : Les 2000 mots les + fréquents dans le corpus #\n",
    "############################################################################\n",
    "\n",
    "books_words = []\n",
    "for w in brown.words(fileids = fic_books):\n",
    "    books_words.append(w.lower())\n",
    "\n",
    "sciences_words = []\n",
    "for w in brown.words(fileids = fic_sciences):\n",
    "    sciences_words.append(w.lower())\n",
    "\n",
    "news_words = []\n",
    "for w in brown.words(fileids = fic_news):\n",
    "    news_words.append(w.lower())\n",
    "\n",
    "tr_words = []\n",
    "for w in brown.words(fileids = train_set):\n",
    "    tr_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_token = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for w in brown.words(fileids = train_set):\n",
    "    word_tokenize(w)\n",
    "    trainset_token.append(w)\n",
    "\n",
    "trainset_token_freq = nltk.FreqDist(trainset_token)\n",
    "\n",
    "#print(trainset_token_freq.most_common(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileid in train_set[categorie]:\n",
    "    nltk.freqDist(trainset_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# on tokenise le test_set & trouve les 2000 mots les plus fréquents\n",
    "#################################################################\n",
    "testset_token = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for i in range (0, len(test_set)):\n",
    "    for w in brown.words(fileids = test_set):\n",
    "        word_tokenize(w)\n",
    "\n",
    "    if w in brown.words(fileids = test_set):\n",
    "        testset_token.append(w)\n",
    "        \n",
    "testset_token_freq = nltk.FreqDist(testset_token)\n",
    "\n",
    "#print(testset_token_freq.most_common(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-249bbc6da34f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'can'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'could'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'may'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'might'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'must'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'will'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'should'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Calcul fréquence Modaux  #\n",
    "############################\n",
    "\n",
    "#pour le train_set\n",
    "\n",
    "from nltk.corpus import brown\n",
    "fdist = nltk.FreqDist(w.lower() for w in brown.fileids(train_set))\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will', 'should']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on cherche le nombre de mots du trainset et de ses partitions\n",
    "nb_words = len(brown.words(train_set))\n",
    "nb_words_news = len(brown.words(train_set['news']))\n",
    "nb_words_books = len(brown.words(train_set['books']))\n",
    "nb_words_sciences = len(brown.words(train_set['sciences']))\n",
    "print (nb_words, nb_words_news, nb_words_books, nb_words_sciences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on cherche le nombre de phrases du trainset et de ses partitions\n",
    "nb_sents = len(brown.sents(train_set))\n",
    "nb_sents_news = len(brown.sents(train_set['news']))\n",
    "nb_sents_books = len(brown.sents(train_set['books']))\n",
    "nb_sents_sciences = len(brown.sents(train_set['sciences']))\n",
    "print (nb_sents, nb_sents_news, nb_sents_books, nb_sents_sciences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on cherche le nombre moyen de mots par phrase\n",
    "#pour ça : nb_words / nb_sents\n",
    "moy_w_sents = int(nb_words/nb_sents)\n",
    "moy_w_sents_news = int(nb_words_news/nb_sents_news)\n",
    "moy_w_sents_books = int(nb_words_books/nb_sents_books)\n",
    "moy_w_sents_sciences = int(nb_words_sciences/nb_sents_sciences)\n",
    "print(moy_w_sents, moy_w_sents_news, moy_w_sents_books, moy_w_sents_sciences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#écriture d'un fichier arff\n",
    "# ici, impossibilité de le faire, mais structure du code créée\n",
    "\n",
    "fichier = open('C:/Users/Garance/Documents/brown_train.arff', 'w')\n",
    "fichier.write('@relation dataset\\n')\n",
    "for mot in keys():\n",
    "    #keys = combinaison des dictionnaires que l'on vient de créer (textes_sci + textes_litt + textes_news)\n",
    "   fichier.write('@attribute ' + str(mot) + ' real\\n')\n",
    "   fichier.write('@attribute type {news, books, sciences}\\n')\n",
    "   fichier.write('@data\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ajout des features\n",
    "fichier = open('C:/Users/Garance/Documents/brown_train.arff', 'a')\n",
    "for ...:\n",
    "    fichier.write()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
